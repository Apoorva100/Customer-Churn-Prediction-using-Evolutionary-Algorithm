import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from scipy.stats import truncnorm
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def initialize_weights(input_dim, hidden_dim):
    # Initialize the weights using a truncated normal distribution
    truncated_norm = truncnorm(a=-1 / np.sqrt(input_dim), b=1 / np.sqrt(input_dim), scale=1)
    W1 = truncated_norm.rvs((input_dim, hidden_dim))
    b1 = np.zeros(hidden_dim)
    W2 = truncated_norm.rvs((hidden_dim, 1))
    b2 = np.zeros(1)
    return [W1, b1, W2, b2]


def forward_propagate(X, weights):
    W1, b1, W2, b2 = weights
    z1 = np.dot(X, W1) + b1
    a1 = sigmoid(z1)
    z2 = np.dot(a1, W2) + b2
    y_pred = sigmoid(z2)
    return y_pred


def calculate_loss(y_true, y_pred):
    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    return np.mean(loss)


def evolutionary_strategy(X, y, num_iterations=10, parent_population_size=2, offspring_population_size=10, sigma=0.1,
                          learning_rate=0.1):
    input_dim = X.shape[1]
    hidden_dim = 2 * input_dim

    weights = initialize_weights(input_dim, hidden_dim)

    # Create parent population using a random function for the 1st generation only
    parent_population = []
    for i in range(parent_population_size):
        w1 = weights[0] + sigma * np.random.randn(*weights[0].shape)
        b1 = weights[1] + sigma * np.random.randn(*weights[1].shape)
        w2 = weights[2] + sigma * np.random.randn(*weights[2].shape)
        b2 = weights[3] + sigma * np.random.randn(*weights[3].shape)
        parent_population.append([w1, b1, w2, b2])

    for iteration in range(num_iterations):

        # Create offspring population
        offspring_population = []
        for i in range(offspring_population_size):
            # Select two parents at random
            parent1 = parent_population[np.random.choice(parent_population_size)]
            parent2 = parent_population[np.random.choice(parent_population_size)]
            # Generate offspring weight vectors by recombining the parents' weights
            w1 = parent1[0] + sigma * np.random.randn(*parent1[0].shape) + sigma * np.random.randn(*parent2[0].shape)
            b1 = parent1[1] + sigma * np.random.randn(*parent1[1].shape) + sigma * np.random.randn(*parent2[1].shape)
            w2 = parent1[2] + sigma * np.random.randn(*parent1[2].shape) + sigma * np.random.randn(*parent2[2].shape)
            b2 = parent1[3] + sigma * np.random.randn(*parent1[3].shape) + sigma * np.random.randn(*parent2[3].shape)
            offspring_population.append([w1, b1, w2, b2])        
        # Evaluate fitness of each weight vector
        fitness_scores = []
        for individual in parent_population + offspring_population:
            y_pred = forward_propagate(X, individual)
            fitness_scores.append(calculate_loss(y, y_pred))
        fitness_scores = np.array(fitness_scores)
        
        # Update weights using the best individuals in the offspring population
        best_offspring = [offspring_population[i] for i in np.argsort(fitness_scores[parent_population_size:])[:parent_population_size]]
        new_weights = []
        for i, weight_matrix in enumerate(weights):
            gradient = np.zeros_like(weight_matrix)
            for individual in best_offspring:
                gradient += (individual[i] - weight_matrix) * fitness_scores[np.argmin(fitness_scores)]
            new_weights.append(weight_matrix + learning_rate/parent_population_size * gradient)
        weights = new_weights
    
    return weights


def plot_graph(train_acc_list, test_acc_list):
    # Plot a graph of the average accuracy against log(number of weight updates) for training and test data
    num_updates = [10**i for i in range(len(train_acc_list))]
    plt.plot(np.log(num_updates), train_acc_list, label='Training Accuracy for ES')
    plt.plot(np.log(num_updates), test_acc_list, label='Test Accuracy ES')
    
    plt.plot(np.log(history.epoch), history.history['accuracy'], label='training accuracy for backpropagation')
    plt.plot(np.log(history.epoch), history.history['val_accuracy'], label='testing accuracy for backpropagation')

    plt.xlabel('Log(Number of Weight Updates)')
    plt.ylabel('Average Accuracy')
    plt.legend()
    plt.show()
    
def show_confusion_matrix(y_true, y_pred, dataset_type):
    cm = pd.crosstab(pd.Series(y_pred), pd.Series(y_true), rownames=['Predicted'], colnames=['Actual'], margins=True)
    return cm


#Read data from csv file
data = pd.read_csv('//content//TestDataFinal - newcsv.csv')
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

train_acc_list = []
test_acc_list = []
cm_list_testing = []
cm_list_training = []


#Repeat the experiment 10 times
for i in range(10):
  #Split data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
  
  # Train the model using Evolutionary Strategy algorithm
  weights = evolutionary_strategy(X_train, y_train)

  # Predict classes for training and test data
  y_train_pred = np.round(forward_propagate(X_train, weights)).flatten()
  y_test_pred = np.round(forward_propagate(X_test, weights)).flatten()

  #Calculate accuracy for training and test data
  train_acc = np.mean(y_train_pred == y_train)
  test_acc = np.mean(y_test_pred == y_test)

  train_acc_list.append(train_acc)
  test_acc_list.append(test_acc)

  #Show confusion matrix for training and test data
  cm_train = show_confusion_matrix(y_train, y_train_pred, 'Training')
  cm_test = show_confusion_matrix(y_test, y_test_pred, 'Test')
  cm_list_testing.append(cm_test)
  cm_list_training.append(cm_train)

#Plot a graph of the average accuracy against log(number of weight updates) for training and test data
plot_graph(train_acc_list, test_acc_list)

cm_concat_train = pd.concat(cm_list_training)
cm_group_train = cm_concat_train.groupby(cm_concat_train.index)
print('-----------------------')
print('Confusion Matrix for Training :')
print(np.round(cm_group_train.mean()))
print('-----------------------')

cm_concat_test = pd.concat(cm_list_testing)
cm_group_test = cm_concat_test.groupby(cm_concat_test.index)
print('Confusion Matrix for Testing :')
print(np.round(cm_group_test.mean()))
print('-----------------------')

